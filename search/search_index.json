{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Our mission","text":"<p>     The modern economy is built on complex digital systems like the cloud.     They are powerful, but painfully hard to operate.     AI agents promise automation, yet today they fail because they lack     operational intelligence:     an understanding of how engineered digital systems respond to actions.   </p> <p>     A simple truth:     the digital world has no physics,     but it obeys rules humans wrote down, which gives it meaning.     APIs, documentation, and traces precisely define how digital systems behave\u2014     what actions mean, what is allowed, and how failures arise.     The cloud is one such world, encoded by tens of thousands of APIs.   </p> <p> Our mission is to synthesize world models for digital systems using AI\u2014     and use them as training grounds to improve AI\u2019s execution intelligence,     starting with the cloud.   </p> <p>     \u2014 The Vera Team   </p>"},{"location":"about/","title":"About","text":"<p>This is the about page.</p>"},{"location":"blogs/cloudagent/cloudagent/","title":"Can AI Agents Replace DevOps? What We Learned Trying to Replace DevOps with AI Agents","text":"February 6, 2026  <p>Cloud infrastructure is the cornerstone of the modern IT industry, yet managing it remains a surprisingly manual and tedious endeavor. From provisioning resources to debugging failures, DevOps teams are constantly battling complexity.</p> <p>In our recent paper, \"Cloud Infrastructure Management in the Age of AI Agents,\" my colleagues and I explore a provocative question: Is it time for AI agents to take the reins?</p> <p>We didn't just want to build a demo; we wanted to understand the fundamental friction between AI models and cloud interfaces. We envision a future where Large Language Models (LLMs) empower autonomous agents to handle these complex workflows. But to get there, we first had to break things.</p>"},{"location":"blogs/cloudagent/cloudagent/#the-setup-a-battle-of-four-agents","title":"The Setup: A Battle of Four Agents","text":"<p>To test the current capabilities of AI, we conducted a preliminary study using four different \"modalities\"\u2014the standard interfaces humans use to manage the cloud. We built four distinct AI agent prototypes to perform tasks on Microsoft Azure:</p> <ol> <li>SDK Agent: Uses Python code (imperative programming).</li> <li>CLI Agent: Uses command-line shell scripts (terminal-based).</li> <li>Infrastructure-as-Code (IaC) Agent: Uses Terraform (declarative configuration).</li> <li>ClickOps Agent: Navigates the web portal visually (just like a human clicking buttons).</li> </ol> <p> Figure 1: Four cloud user interaction modalities with simplified code snippets. </p>"},{"location":"blogs/cloudagent/cloudagent/#insight-1-speed-vs-visibility-the-blindness-of-code","title":"Insight 1: Speed vs. Visibility (The \"Blindness\" of Code)","text":"<p>We pitted these agents against each other in three rounds of tasks: Provisioning, Updates, and Monitoring. The results revealed a massive trade-off between execution speed and state awareness.</p> <p>The CLI agent was the speed demon of the group. For provisioning tasks, it was highly efficient, completing tasks in an average of 1.6 steps. Because it operates via text commands, it could often do in one line what took the ClickOps agent 30 times as many steps to achieve. The ClickOps agent was excruciatingly slow because it had to wait for page loads and navigate complex menus, just like a human user would.</p> <p>However, speed isn't everything. When we moved to Update tasks (like changing a VM's disk), the \"blindness\" of the coding agents became a liability. The CLI and SDK agents struggled because they couldn't easily \"see\" the current configuration of the cloud. They often failed because they needed to query the state before modifying it, introducing extra steps where errors could creep in.</p> <p>In contrast, the ClickOps agent actually performed better here (67% success rate vs. 33% for IaC). Why? Because the web portal visually presented the existing configuration. The agent could \"see\" the current state on the screen, reducing hallucinations and errors during modification.</p>"},{"location":"blogs/cloudagent/cloudagent/#insight-2-the-wrong-tool-fallacy","title":"Insight 2: The \"Wrong Tool\" Fallacy","text":"<p>One of our most critical findings was that some industry-standard tools are fundamentally ill-suited for AI agents in certain contexts.</p> <p>Take Infrastructure-as-Code (IaC). It is the gold standard for human DevOps because it is declarative\u2014you describe what you want, not how to get there. But for an AI agent trying to Monitor a system, IaC was a disaster.</p> <p>When we asked the IaC agent to check the health of a system, it failed significantly (40% success rate). IaC is designed to define infrastructure, not to query real-time telemetry. The agent hallucinated non-existent commands because it was trying to force a square peg into a round hole. Meanwhile, the Web/ClickOps agent excelled at monitoring because it could simply look at the graphs and dashboards already built into the Azure portal.</p> <p> Figure 2: The radar chart summarizing agent performance across different modalities. </p>"},{"location":"blogs/cloudagent/cloudagent/#the-path-forward-a-new-architecture","title":"The Path Forward: A New Architecture","text":"<p>Our experiments highlight that simply connecting an LLM to a cloud shell isn't enough. We need a more sophisticated architecture that embraces these trade-offs.</p> <p>We propose a new roadmap for Agentic Cloud Management involving three key pillars:</p> <ol> <li>Multi-Agent Orchestration: We shouldn't force one agent to do everything. We envision a system that routes tasks to the best \"expert.\" It would dispatch a CLI agent for fast provisioning but switch to a ClickOps agent for visual debugging or monitoring.</li> <li>Sandboxed Exploration (\"Cloud Gyms\"): Agents need a safe space to fail. We propose separating workflows into \"Exploration\" and \"Exploitation\" phases. Agents should test their strategies in a sandboxed \"gym\"\u2014a simulated or non-production environment\u2014before touching live infrastructure.</li> <li>Workflow Memory &amp; Guardrails: Once an agent figures out a complex task in the sandbox, it shouldn't have to relearn it. It should \"cache\" that workflow into a memory bank for future use. Furthermore, we need strict \"human-in-the-loop\" guardrails to prevent costly or dangerous mistakes before they happen.</li> </ol> <p> Figure 3: Envisioned agentic system architecture and workflow. </p>"},{"location":"blogs/cloudagent/cloudagent/#conclusion","title":"Conclusion","text":"<p>Cloud management is ripe for automation, but our research shows that the interface matters just as much as the model. By combining the speed of CLI, the stability of IaC, and the visibility of ClickOps, we can build the next generation of autonomous cloud engineers.</p> <p>For more details, check out our full paper in the ACM SIGOPS Operating Systems Review.</p> Authors:     Zhenning Yang, Archit Bhatnagar, Yiming Qiu, Tongyuan Miao, Patrick Tser Jern Kon, Yunming Xiao, Yibo Huang, Martin Casado, Ang Chen    Affiliations:     University of Michigan, UC Berkeley, Andreessen Horowitz    Paper link:        https://dl.acm.org/doi/abs/10.1145/3759441.3759443"},{"location":"blogs/cloudemu/cloudemu/","title":"No More Manual Mocks: A Case for Learned Cloud Emulators","text":"February 6, 2026  <p>DevOps engineers face a constant dilemma: testing against the real cloud is expensive and slow, but testing locally requires emulators that often lack features or behave differently than the real thing.</p> <p>Existing emulators (like the popular LocalStack) rely on manual engineering. We observe that this is a Sisyphean task\u2014AWS alone has over 240 services, and human developers struggle to keep up with the constant stream of new APIs and updates. As a result, coverage is often spotty; for example, we found that existing tools cover only ~11% of the AWS Network Firewall APIs.</p> <p>In our paper \"A Case for Learned Cloud Emulators,\" we propose a paradigm shift: instead of hand-coding mocks, why not use Large Language Models (LLMs) to learn the emulation logic directly from cloud documentation?</p>"},{"location":"blogs/cloudemu/cloudemu/#the-learned-emulator-approach","title":"The \"Learned Emulator\" Approach","text":"<p>Simply asking an LLM to \"write a cloud emulator\" results in buggy, hallucinated code. To solve this, we introduce a neuro-symbolic workflow that constrains the AI using formal abstractions. We view each cloud resource not just as code, but as a formal State Machine (SM).</p> <p> Figure 1: The grammar for specifying an emulator. </p> <p>Our workflow consists of three key stages:</p> <ol> <li>Documentation Wrangling: We automatically scrape and index massive cloud documentation (like AWS PDFs), organizing it into resource-specific contexts to overcome LLM context window limits.</li> <li>State Machine (SM) Extraction: Instead of generating raw Python code immediately, we task the LLM with extracting a formal State Machine specification from the docs. This captures the resource's states (e.g., \"available,\" \"pending\") and valid transitions (API calls like <code>CreateVpc</code>), enforcing logic and dependencies that unstructured code generation misses.</li> <li>Automated Alignment: To ensure the emulator behaves exactly like the real cloud, we run traces against both our generated emulator and the actual cloud. We detect discrepancies (e.g., an error code mismatch) and feed them back to the LLM to patch the logic automatically.</li> </ol> <p> Figure 2: Our envisioned workflow. </p>"},{"location":"blogs/cloudemu/cloudemu/#preliminary-results","title":"Preliminary Results","text":"<p>We built a prototype and the results highlight significant advantages over current manual methods:</p> <ul> <li>Better Coverage: In our preliminary tests, we achieved 100% API coverage for the AWS Network Firewall service, compared to just 11% for the state-of-the-art manual emulator.</li> <li>Accuracy: By modeling resources as State Machines, we prevent common logic errors. As shown below, our SM-based approach maintains high accuracy during state updates, whereas a direct-to-code (D2C) LLM approach often fails completely (0% accuracy) because it loses track of resource dependencies.</li> </ul> <p> Figure 3: Accuracy of learned emulators across scenarios. </p> <p>We also used our extracted state machines to quantify the complexity of different cloud services, counting the number of state transitions inherent to services like EC2 versus DynamoDB.</p> <p> Figure 4: CDF of SM complexity across services. </p>"},{"location":"blogs/cloudemu/cloudemu/#the-future-a-cloud-gym","title":"The Future: A \"Cloud Gym\"","text":"<p>Beyond just testing, we believe this technology opens new doors. A high-fidelity, zero-cost emulator could serve as a \"Cloud Gym\"\u2014a training ground for AI agents to learn how to manage cloud infrastructure without the risk of racking up a massive bill.</p> <p>By turning static documentation into executable logic, we hope to finally solve the bottleneck of cloud development velocity.</p> <p>For more details, check out our full paper in the HotNets '25 proceedings.</p> Authors:     Archit Bhatnagar, Yiming Qiu, Sarah McClure, Sylvia Ratnasamy, Ang Chen    Affiliations:     University of Michigan, The University of Hong Kong, UC Berkeley    Paper link:        https://dl.acm.org/doi/10.1145/3772356.3772404"}]}